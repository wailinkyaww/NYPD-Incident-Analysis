---
title: "NYPD Shooting Incident - Analysis Report"
author: "Wai Lin Kyaw"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

The dataset contains all the shooting incidents that occurred in NYC from 1st Jan of 2006 all the way to end of 2022. The dataset is published by the New York Police Department and it is still being updated regularly.

Each record contains followings:

- Date & time of the incident occurred
- Location of the incident
- Area/City where the incident occurred
- Victim information
- Perpetrator information

We will explore and analyse the all the aspects except the location.

```{r, echo=FALSE, warning=FALSE}
suppressPackageStartupMessages({
  library(tidyverse)
  library(conflicted)
  library(ggplot2)
  library(lubridate)
  library(caret)
})
```

## Importing dataset 

Dataset is already downloaded in the workspace. 
We will import the dataset and do basic inspection.
```{r, echo=FALSE, message=FALSE}
dataset_url <- './NYPD-Shooting-Incident.csv'

incidents <- read_csv(dataset_url)
```

```{r, echo=FALSE, message=FALSE}
n_samples <- nrow(incidents)
n_variables <- ncol(incidents)
variables <- colnames(incidents)

cat("Number of samples:", n_samples)
cat("Number of variables:", n_variables)
cat("Variables:")
variables
```

## Wrangling & cleaning dataset

We have about 21 variables here.
We can get rid of the ones that we don't really need so that we can focus on what's important.

First, let's remove the variables related to location of the incident except borough.

```{r, message=FALSE}
incidents <- incidents %>%
  select(-c("PRECINCT", 
           "JURISDICTION_CODE", 
           "LOC_CLASSFCTN_DESC",
           "LOCATION_DESC",
           "X_COORD_CD",
           "Y_COORD_CD",
           "Latitude",
           "Longitude",
           "Lon_Lat"
           ))
```

We can inspect the dataset to observe NA and null values.
```{r, message=FALSE}
na_cols <- colSums(is.na(incidents) | is.null(incidents))
na_cols[na_cols > 0]
```

As we can see, above 4 variables have a lot of missing values. It's important to impute these with appropriate values so that, when we do the analysis, it will have proper meaning and easy to interpret.

We replace those `NA` values with `Unknown`.
And we will remove the columns where more than 75% of the rows have `NA`.

```{r, echo=FALSE, message=FALSE}
# This will remove the location of the occurrence description
# The statement is designed to remove the row that has NA in 75% of the rows.
# Anyway, this kind of data won't help much.
incidents <- incidents[, colSums(is.na(incidents)) < nrow(incidents) * 0.75]

# Using unknown for the rest of the NA values
incidents[is.na(incidents)] <- "Unknown"

# replace (null) values with Unknown.
# this is because the original dataset contains (null) as strings
incidents$PERP_SEX[incidents$PERP_SEX == "(null)"] <- "Unknown"
incidents$PERP_RACE[incidents$PERP_RACE == "(null)"] <- "Unknown" 
incidents$PERP_AGE_GROUP[incidents$PERP_AGE_GROUP == "(null)"] <- "Unknown"

# standardizing the casing for Unknown.
incidents$PERP_SEX[incidents$PERP_SEX == "UNKNOWN"] <- "Unknown"
incidents$PERP_RACE[incidents$PERP_RACE == "UNKNOWN"] <- "Unknown" 
incidents$PERP_AGE_GROUP[incidents$PERP_AGE_GROUP == "UNKNOWN"] <- "Unknown"
```
We are left with the tibble of 11 variables. It is a good time to transform the data to more relevant data types. We can typecast followings into factor (categorical value).

- boro (renaming this to borough)
- location description
- age group
- sex
- race

```{r, echo=FALSE, message=FALSE}
incidents <- incidents %>%
  rename(BOROUGH = BORO) %>%
  mutate(across(
      c(BOROUGH,
        PERP_AGE_GROUP, 
        PERP_RACE, 
        PERP_SEX, 
        VIC_AGE_GROUP, 
        VIC_RACE, 
        VIC_SEX),
      factor)
  )
```

## Exploratory Data Analysis

We are going to visualize the following three key aspects of the incidents:

- Shootings by borough
- Time series of shooting incidents
- Victim age by incident

```{r, echo=FALSE, message=FALSE}
ggplot(incidents, aes(x = BOROUGH, fill = BOROUGH)) +
  geom_bar(width = 0.5) +
  labs(
    title = "Number of Incidents by Borough in NYC",
    x = "Borough",
    y = "Number of Incidents"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5)
  )
```

From this we can see that **Brooklyn** has the highest number of incidents followed by `Bronx`.
`Queens` and `Manhattan` has the same amount of incidents.

I would say `Staten Island` has smaller number of incidents compared to the rest.

It is interesting that the rate in `Brooklyn` is super high while the `Staten Island`, only a bridge away has small number of incidents.

```{r, echo=FALSE, message=FALSE}
# We want to visualize the incidents occurred by the date in time series structure. 
# To do that, we need to group the incidents by the date and calculate the count first.
incidents$OCCUR_DATE <- mdy(incidents$OCCUR_DATE)

incidents$month <- floor_date(incidents$OCCUR_DATE, "month")

incident_counts <- incidents %>%
  group_by(month) %>%
  summarise(count = n())

ggplot(incident_counts, aes(x = month, y = count)) +
  geom_line(aes(col = "lightgreen")) +
  scale_color_manual(values = c("lightgreen"="#00ba38")) +
  theme_minimal() +
  labs(
    title = "Time Series - Number of Incidents Over Time",
    x = "\nDate (Month)",
    y = "Number of Incidents"
  ) +
  scale_x_date(date_labels = "%Y", date_breaks = "year") + 
  theme(
    axis.text.x = element_text(angle = 60, vjust = 0.5, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  )
```

Here are some findings from this time series - visualization.

1. Ongoing incident rate on these 5 years term is same as the time back to 2006 all way to 2013.
2. From 2013 to 2020, it decreases bit by bit reaching the near bottom in 2020.
3. Interestingly enough, there was a spike during 2021. This is the lock down period when in Covid19 pandemic. Logically, people stay home, and the ongoing pandemic, it should've been lower than usual.

It is declining since after mid 2023. I hope this will continue.

```{r, echo=FALSE, message=FALSE}
ggplot(incidents, aes(x = VIC_AGE_GROUP, fill = VIC_AGE_GROUP)) + 
  geom_bar(width = 0.35) +
  labs(
    title = "Number of Incidents by victim age",
    x = "Victim age group",
    y = "Number of Incidents"
  ) +
  theme(
    axis.text.x = element_text(angle = 60, vjust = 0.6),
    plot.title = element_text(hjust = 0.5)
  )
```

As we can see from the last, those between 25-44 and 18-24 age groups are the most prone to the incidents.


## Building a Model

Let's split the data into training and testing dataset first.
```{r, echo=FALSE, message=FALSE}
set.seed(42)

train_index <- createDataPartition(incidents$STATISTICAL_MURDER_FLAG, p=0.8, list=FALSE)
train_incidents <- incidents[train_index,]
test_incidents <- incidents[-train_index,]
```

Now, we will build a machine learning model to predict whether is not a case is an instance of statistical murder or not. 

To get a probability, we will use logistic regression which is design to squeeze the Infinite possibilities into the range of 0 and 1.

```{r, echo=FALSE, message=FALSE}
model = glm(
  STATISTICAL_MURDER_FLAG ~ 
    BOROUGH + 
    OCCUR_DATE + OCCUR_TIME +
    PERP_AGE_GROUP + PERP_SEX + PERP_RACE +
    VIC_AGE_GROUP + VIC_SEX + VIC_RACE,
  data=train_incidents,
  family = binomial
)

summary(model)
```

## Conclusion & Biases

We have built the logistic regression model here to predict the statistical murder case or not.

As next steps we could add 
  - testing 
  - evaluation
  - various metrics 
about the model that we just built. 

Inspecting and looking through the dataset, having some exploratory analysis through visualization is a good approach to get ourselves to gain more exposure with the data that we are working with.

We should always do that before we dive into the modeling first.

**Potential Biases**

Here when we build the model, we are generalizing most of the features that we have.
Although this seems to work, it might not be the case in practical.

For example, the factors contributing to the cases might be different during covid crisis compared to the ones before and after.

Or even there might be some other aspects that doesn't even included in the dataset.

Trying out with the real scenarios, talking with the industry experts and verify with the facts can reduce the potential biases.